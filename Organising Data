# Import necessary libraries
import json
import pdfplumber
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec

# Load data from the JSON database
def load_data(file):
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

# Write data to a JSON file
def write_data(file, data):
    with open(file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

# Define the file path of your JSON database
database_file = "/Users/kylenabors/Documents/GitHub/MS-Thesis/Training Data/Fed Data/fed_database.json"

# Load the data from the JSON database
database_data = load_data(database_file)

# Extract file paths from the database data
files = [entry["path"] for entry in database_data if "path" in entry]

# List to hold processed segments from the PDF files
final = []

# Process each PDF file
for file in files:
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            segments = text.split(". ")
            for segment in segments:
                segment = simple_preprocess(segment, deacc=True)
                if len(segment) > 5:
                    final.append(segment)

# Write the processed data to a JSON file
write_data("/Users/kylenabors/Documents/GitHub/MS-Thesis/Training Data/Fed Data/fed_data.json", final)

# Load the processed data
segments = load_data("/Users/kylenabors/Documents/GitHub/MS-Thesis/Training Data/Fed Data/fed_data.json")

# Build a Word2Vec model from the processed data
model = Word2Vec(segments, min_count=5, workers=3, window=3, sg=1)

# Save the Word2Vec model
model.save("/Users/kylenabors/Documents/GitHub/MS-Thesis/Models/fed_word2vec.model")

# Load the saved Word2Vec model
model = Word2Vec.load("/Users/kylenabors/Documents/GitHub/MS-Thesis/Models/fed_word2vec.model")

# Find and print the top 20 words similar to a given list of keywords
keywords = ["interest", "rate", "market"]  # define your list of keywords here
for keyword in keywords:
    res = model.wv.similar_by_word(keyword, topn=20)
    print(f"For the keyword '{keyword}', the top 20 similar words are:")
    for item in res:
        print(item)
    print("\n")

# Define search words for training data
search_words = ["public", "market", "business", "raise", "lower", "interest", "rate"]

# List to hold training data
train_data = []

# Filter the processed data based on search words
for segment in segments:
    if any(word in segment for word in search_words):
        segment = " ".join(segment)
        train_data.append((segment, 1))

# Write the training data to a JSON file
write_data("/Users/kylenabors/Documents/GitHub/MS-Thesis/Training Data/Fed Data/fed_data_word_train.json", train_data)

# Load the training data
interest_train = load_data("/Users/kylenabors/Documents/GitHub/MS-Thesis/Training Data/Fed Data/fed_data_word_train.json")

# Print the length of the training data
print(len(interest_train))
