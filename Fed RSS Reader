import os
import requests
import feedparser
from bs4 import BeautifulSoup
import urllib

# URL of the RSS feed
rss_url = "https://www.federalreserve.gov/feeds/press_all.xml"

# Directory to save the downloaded files
output_dir = "downloads"

# Create the output directory if it doesn't exist
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

def download_file(url, path):
    """Download a file from a URL and save it to a local path."""
    response = requests.get(url)
    filename = os.path.join(path, url.split("/")[-1])
    with open(filename, "wb") as file:
        file.write(response.content)

# Parse the RSS feed
feed = feedparser.parse(rss_url)

# Iterate over the entries in the feed
for entry in feed.entries:
    # Parse the HTML content of the entry
    soup = BeautifulSoup(entry.summary, 'html.parser')
    
    # Find all the <a> tags with href attributes ending in '.pdf'
    pdf_links = soup.find_all('a', href=lambda href: href.endswith('.pdf'))
    all_links = soup.find_all()
    
    for all_link in all_links:
        url_all = all_link['href']
        if url_all.startswith('/'):
            url_all = 'federalreserve.gov' + url_all
        print("Found", {url_all}) 
        download_file(url_all, output_dir)
        
    # Download each PDF
    for pdf_link in pdf_links:
        url = pdf_link['href']
        if url.startswith('/'):
            url = 'federalreserve.gov' + url
        print(f"Found {url}")
        download_file(url, output_dir)
