# Import necessary libraries
import json
import pdfplumber
import csv
from collections import defaultdict, Counter
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec
import spacy
from multiprocessing import Pool, cpu_count

# Load data from the JSON database
def load_data(file):
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

# Write data to a JSON file
def write_data(file, data):
    with open(file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

def process_file(file_info):
    nlp = spacy.load("en_core_web_sm")
    file, year_month, doc_type = file_info
    segments_for_file = []
    keyword_freq_for_file = defaultdict(Counter)
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            doc = nlp(text)
            segments = text.split(". ")
            for segment in segments:
                segment = [token.lemma_ for token in doc if token.pos_ not in ['PROPN', 'PUNCT'] and len(token.orth_) > 2]  
                if len(segment) > 5:
                    segments_for_file.append(segment)
                    for word in segment:
                        keyword_freq_for_file[year_month][doc_type][word] += 1
    return segments_for_file, keyword_freq_for_file

# List to hold processed segments from the PDF files
final = []
keyword_freq_ts = defaultdict(lambda: defaultdict(Counter))

if __name__ == '__main__':
    # Define the file path of your JSON database
    database_file = "/Users/kylenabors/Documents/GitHub/MS-Thesis/Database/fed_database.json"

    # Load the data from the JSON database
    database_data = load_data(database_file)

    # Define accepted types
    accepted_types = ['Beige Book']  # replace these with actual types

    # Extract file paths from the database data
    files = [(entry["path"], entry["date"][:7], entry["type"]) for entry in database_data if "path" in entry and entry["type"] in accepted_types] 

    # Process each PDF file
    pool = Pool(cpu_count())
    results = pool.map(process_file, files)

    # Merge the results
    for segments_for_file, keyword_freq_for_file in results:
        final.extend(segments_for_file)
        for year_month, doc_types in keyword_freq_for_file.items():
            for doc_type, word_counts in doc_types.items():
                for keyword, freq in word_counts.items():
                    keyword_freq_ts[year_month][doc_type][keyword] += freq

    print(len(final)) 
    
    pool.close()
    pool.join()

# Write the processed data to a JSON file
write_data("/Users/kylenabors/Documents/GitHub/MS-Thesis/Database/Fed Data/fed_data.json", final)

# Load the processed data
segments = load_data("/Users/kylenabors/Documents/GitHub/MS-Thesis/Database/Fed Data/fed_data.json")

# Create an empty model
model = Word2Vec(min_count=15, workers=10, window=20, epochs=2, sg=1)

# Build the vocabulary
model.build_vocab(segments)

# Train the model
model.train(segments, total_examples=model.corpus_count, epochs=model.epochs)


# Save the Word2Vec model
model.save("/Users/kylenabors/Documents/GitHub/MS-Thesis/Models/Fed Models/fed_word2vec.model")

# Load the saved Word2Vec model
model = Word2Vec.load("/Users/kylenabors/Documents/GitHub/MS-Thesis/Models/Fed Models/fed_word2vec.model")

# Find and print the top 10 words similar to a given list of keywords
keywords = ["raise", "lower", "interest", "rate", "inflation"]  # define your list of keywords here
for keyword in keywords:
    res = model.wv.similar_by_word(keyword, topn=10)
    print(f"For the keyword '{keyword}', the top 10 similar words are:")
    for item in res:
        print(item)
    print("\n")

# List to hold training data
train_data = []
keyword_dict = {keyword: i+1 for i, keyword in enumerate(keywords)}
keyword_freq = Counter()

# Filter the processed data based on search words
for segment in segments:
    for word in keywords:
        if word in segment:
            segment = " ".join(segment)
            train_data.append((segment, keyword_dict[word]))
            keyword_freq[word] += 1

# Write the training data to a JSON file
write_data("/Users/kylenabors/Documents/GitHub/MS-Thesis/Database/Fed Data/fed_data_train.json", train_data)

# Write keyword information to a CSV file
with open("/Users/kylenabors/Documents/GitHub/MS-Thesis/Database/Fed Data/keyword_info.csv", "w", newline="") as csvfile:
    fieldnames = ['Keyword', 'Number', 'Frequency']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for keyword, number in keyword_dict.items():
        writer.writerow({'Keyword': keyword, 'Number': number, 'Frequency': keyword_freq[keyword]})

# Write keyword frequency by year-month and type to a CSV file
with open("/Users/kylenabors/Documents/GitHub/MS-Thesis/Database/Fed Data/keyword_info_ts.csv", "w", newline="") as csvfile:
    fieldnames = ['Year-Month', 'Type', 'Keyword', 'Frequency']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for year_month, doc_types in keyword_freq_ts.items():
        for doc_type, word_counts in doc_types.items():
            for keyword, freq in word_counts.items():
                if keyword in keywords:  # only write the selected keywords to the CSV
                    writer.writerow({'Year-Month': year_month, 'Type': doc_type, 'Keyword': keyword, 'Frequency': freq})
