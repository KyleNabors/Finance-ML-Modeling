# Import necessary libraries
import sys
import os
import json
import pdfplumber
import csv
from collections import defaultdict, Counter
import spacy
import nltk
from nltk.corpus import words, stopwords
import pandas as pd
import re

## for data
import numpy as np
from sklearn import metrics, manifold
## for plotting
import matplotlib.pyplot as plt
import seaborn as sns
## for w2v
import gensim
import gensim.downloader as gensim_api
## for bert
import transformers

nlp = spacy.load("en_core_web_lg")

nltk.download("stopwords") 
stopwords = set(stopwords.words('english'))
#stopwords = stopwords.words('english')

nltk.download('words')
hyphenated_words = set(word for word in words.words() if '-' in word)
all_words = set(word for word in words.words())

#Find and import config file
config_path = os.getcwd()
sys.path.append(config_path)
import config

#Variables, Paramaters, and Pathnames needed for this script
database_file = config.database
database_folder = config.database_folder

# Load data from the JSON database
def load_data(file):
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

# Write data to a JSON file
def write_data(file, data):
    with open(file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

# Load the data from the JSON database
database_data = load_data(database_file)

# Define accepted types
accepted_types = ['Beige Book',
                  ]

#'JP Morgan Annual Reports',
#'Citi Annual Reports'
#'Financial Stability Reports'
#'Beige Book'
#'Monetary Policy Report'

Model = 'Beige Book'
Model_Subfolder = f'/Fed Texts/{Model}'
Model_Folder = config.texts
Model_Folder = Model_Folder + Model_Subfolder
print(Model_Folder)

# Extract file paths from the database data
files = [(entry["path"], entry["date"][:10], entry["type"]) for entry in database_data if "path" in entry and entry["type"] in accepted_types] 

# Sort files by date
files.sort(key=lambda x: x[1])  # sort by year_month_day

# Specify the percentage of files you want to process
scale = config.scale
percentage_to_process = scale 
files_to_process = files[::int(1 / percentage_to_process)]

# Specify the year and month you want to start and end processing files from
start_year_month_day = '2006-12-31'
end_year_month_day = '2023-12-31'

# Only process files from the selected year and month range
files_to_process = [file for file in files_to_process if start_year_month_day <= file[1] <= end_year_month_day]

# List to hold processed segments from the PDF files
final = []
remove_words = ["report", "appendix", "table", 'contents', 'overview', 'reserve', 'congress', 'board', 'federal', 'senate', 
                'seventh', 'document', 'beige', 'book', 'commentary', 'summary', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelveth',
                "chase", "citi", 'fi', 'par', 'committee', 'chairman', 
                'janurary', 'feburary', 'march', 'april', 'may', 'june', 
                'july', 'august', 'september', 'october', 'november', 'december',
                'year', 'month',
                'noted', 'since', 'said', 'also', 'one', 'first', 'six', 'third', 
                'united', 'city', 'district', 'york', 'new york', 'boston', 'san', 'francisco', 'atlanta', 'chicago', 'virgin', 'northern', 'montana',
                'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

csv_out = []

# Precompile the regular expression pattern
pattern = re.compile(r'[^a-zA-z.,!?/:;\"\'\w\s]')

# Process each PDF file

# Global Counter object to keep track of word frequencies
global_word_counter = Counter()

keyword_freq_ts = defaultdict(lambda: defaultdict(Counter))
keyword_freq_ts2 = defaultdict(lambda: defaultdict(Counter))
for file, year_month_day, doc_type in files_to_process:
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text(layout = True)
            
            text = text.lower()

            # Handle hyphenation in one go
            text = ' '.join(word.replace('-', '') if '-' in word and word not in hyphenated_words else word for word in text.split())
            
            # Remove numbers in one go
            text = pattern.sub('', text)

            segments = text.split('\n\n')
            
            for segment in segments:
                # Remove unwanted words and extra spaces
                segment_words = [word.strip() for word in segment.split() if word not in remove_words and word not in stopwords and word in all_words]
                segment = ' '.join(segment_words)
                if 20 < len(segment) < 10000:
                    final.append(segment)
                    csv_out.append([year_month_day, doc_type, segment])
                    keyword_freq_ts[year_month_day][doc_type].update(segment_words)
                    global_word_counter.update(segment_words)
                    
print(len(final))

# Sort words by frequency
sorted_word_freq = sorted(global_word_counter.items(), key=lambda x: x[1], reverse=True)

# Write the processed data to a JSON file
write_data(f"{Model_Folder}/{Model}_texts.json", final)
write_data(f"{Model_Folder}/keyword_freq_ts_{Model}.json", keyword_freq_ts)

df_csv_out = pd.DataFrame(csv_out, columns=["date", "type", "segment"])
df_csv_out.to_csv(f"{Model_Folder}/{Model}_texts.csv", index=True)

# Write sorted words and their frequencies to a file
with open(f"{Model_Folder}/{Model}_word_freq.csv", "w", newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['Word', 'Frequency'])  # Column headers
    for word, freq in sorted_word_freq:
        writer.writerow([word, freq])
        
        