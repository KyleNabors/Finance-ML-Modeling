# Import necessary libraries
import sys
import os
import json
import csv
from collections import defaultdict, Counter
import spacy
import nltk
from nltk.corpus import words, stopwords
import pandas as pd
import re
import matplotlib.pyplot as plt
import gensim
from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize
import time

#Json read and write functions
def load_data(file):
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def write_data(file, data):
    with open(file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

nlp = spacy.load("en_core_web_lg")
nltk.download("stopwords") 
stopwords_en = nltk.corpus.stopwords.words('english')
stopwords_sp = nltk.corpus.stopwords.words('spanish')
stopwords_fr = nltk.corpus.stopwords.words('french')
stopwords_it = nltk.corpus.stopwords.words('italian')
stopwords = stopwords_en + stopwords_sp + stopwords_fr + stopwords_it

nltk.download('words')
hyphenated_words = set(word for word in words.words() if '-' in word)
all_words = nltk.corpus.words.words('en')
#all_words = set(word for word in words.words())

#Find and import config file
config_path = os.getcwd()
sys.path.append(config_path)
import config

#Variables, Paramaters, and Pathnames needed for this script
database_file = config.database
database_folder = config.database_folder
Model_Folder = config.texts

database_data = load_data(database_file)

#Load Model Parameters
Body = config.Body
Model = config.Model
accepted_types = config.accepted_types
Model_Subfolder = f'/{Body} Texts/{Model}'
Model_Folder = Model_Folder + Model_Subfolder


#Fed Speeches 
if Body == "Fed":
    files = pd.read_csv(f"/Users/kylenabors/Documents/Database/Training Data/Fed/Speeches/fed_speeches_1995_2023.csv", encoding='UTF-8')

#ECB Speeches 
if Body == "ECB":
    files = pd.read_csv('/Users/kylenabors/Documents/Database/Training Data/ECB/Speeches/all_ECB_speeches.csv', sep = "|", quote = "", fill = TRUE, header = TRUE, encoding = "UTF-8", stringsAsFactors = FALSE)
    files['contents'] = files['contents'].astype(str)
# Specify the year and month you want to start and end processing files from
# start_year_month_day = '2002-06-01'
# end_year_month_day = '2023-12-31'

# if Body == "Fed":
#     files['date'] = files['date'].astype(int)
#     files['date'] = pd.to_datetime(files['date'], format='%Y%m%d')

# files = files[files['date'] >= start_year_month_day]
# files = files[files['date'] <= end_year_month_day]

# files['date'] = files['date'].astype(str)

final = []
csv_out = []

pattern = re.compile(r'[^a-zA-z.,!?/:;\"\'\w\s]')

# Global Counter object to keep track of word frequencies
global_word_counter = Counter()

keyword_freq_ts = defaultdict(lambda: defaultdict(Counter))
keyword_freq_ts2 = defaultdict(lambda: defaultdict(Counter))

years = []
for index, row in files.iterrows():
    
    if Body == "Fed":
        year_month_day = row['date']
        doc_type = row['speaker']
        text = row['text']
        title = row['title']
        doc_year = row['year']
        
    if Body == "ECB":
        year_month_day = row['date']
        doc_type = row['speakers']
        text = row['contents']
        title  = row['title']

    text = text.casefold()
    text = re.sub(r"[^A-Za-z0-9.,!?/:;\s]+", "", text)
    
    segments = sent_tokenize(text, language='english')

    
    for segment in segments:
        if 30 < len(segment) < 100000:
            segment_words = word_tokenize(segment, language='english')
            segment = ' '.join(segment_words)
            
            if 30 < len(segment) < 200000:
                final.append(segment)
                csv_out.append([year_month_day, title,  doc_type, segment])
                keyword_freq_ts[year_month_day][doc_type].update(segment_words)
                global_word_counter.update(segment_words)
            
print(len(final))

# Sort words by frequency
sorted_word_freq = sorted(global_word_counter.items(), key=lambda x: x[1], reverse=True)

# Write the processed data to a JSON file
write_data(f"{Model_Folder}/{Model}_texts.json", final)
write_data(f"{Model_Folder}/keyword_freq_ts_{Model}.json", keyword_freq_ts)

df_csv_out = pd.DataFrame(csv_out, columns=["date", "title", "type", "segment"])
df_csv_out.to_csv(f"{Model_Folder}/{Model}_texts.csv", index=True)

# Write sorted words and their frequencies to a file
with open(f"{Model_Folder}/{Model}_word_freq.csv", "w", newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['Word', 'Frequency'])
    for word, freq in sorted_word_freq:
        writer.writerow([word, freq])




