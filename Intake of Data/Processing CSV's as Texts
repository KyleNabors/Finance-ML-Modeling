# Import necessary libraries
import sys
import os
import json
import csv
from collections import defaultdict, Counter
import spacy
import nltk
from nltk.corpus import words, stopwords
import pandas as pd
import re

## for plotting
import matplotlib.pyplot as plt
## for w2v
import gensim

nltk.download('punkt')

nlp = spacy.load("en_core_web_lg")

nltk.download("stopwords") 
stopwords = set(stopwords.words('english'))

nltk.download('words')
hyphenated_words = set(word for word in words.words() if '-' in word)
all_words = set(word for word in words.words())

#Find and import config file
config_path = os.getcwd()
sys.path.append(config_path)
import config

#Variables, Paramaters, and Pathnames needed for this script
database_file = config.database
database_folder = config.database_folder

# Load data from the JSON database
def load_data(file):
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

# Write data to a JSON file
def write_data(file, data):
    with open(file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

# Load the data from the JSON database
database_data = load_data(database_file)

Body = config.Body
Model = config.Model
accepted_types = config.accepted_types
Model_Subfolder = f'/{Body} Texts/{Model}'
Model_Folder = config.texts
Model_Folder = Model_Folder + Model_Subfolder
print(Model_Folder)

files = pd.read_csv(f"{Model_Folder}/{Model}_texts.csv")  
#files = pd.read_csv(f"{Model_Folder}/{Model}_texts.csv", sep=";", encoding='cp1252')  
#files = pd.read_csv('/Users/kylenabors/Documents/Database/Training Data/Fed/Press Confrences/press_conference_transcript.csv')
#files = pd.DataFrame(files, columns=['document_kind', 'meeting_date', 'release_date', 'text', 'url'])

# Specify the year and month you want to start and end processing files from
start_year_month_day = '2006-12-31'
end_year_month_day = '2023-12-31'

files = files[files['date'] >= start_year_month_day]
files = files[files['date'] <= end_year_month_day]

# List to hold processed segments from the PDF files
final = []
remove_words = ["report", "appendix", "table", 'contents', 'overview', 'reserve', 'congress', 'board', 'federal', 'senate', 
                'seventh', 'document', 'beige', 'book', 'commentary', 'summary', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelveth',
                "chase", "citi", 'fi', 'par', 'committee', 'chairman', 
                'janurary', 'feburary', 'march', 'april', 'may', 'june', 
                'july', 'august', 'september', 'october', 'november', 'december',
                'year', 'month',
                'noted', 'since', 'said', 'also', 'one', 'first', 'six', 'third', 
                'united', 'city', 'district', 'york', 'new york', 'boston', 'san', 'francisco', 'atlanta', 'chicago', 'virgin', 'northern', 'montana',
                'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

csv_out = []

# Precompile the regular expression pattern
pattern = re.compile(r'[^a-zA-z.,!?/:;\"\'\w\s]')

# Process each PDF file

# Global Counter object to keep track of word frequencies
global_word_counter = Counter()

keyword_freq_ts = defaultdict(lambda: defaultdict(Counter))
keyword_freq_ts2 = defaultdict(lambda: defaultdict(Counter))
for index, row in files.iterrows():
    year_month_day = row['date']
    doc_type = row['president']
    text = row['firstPart']
    text = text.lower()
    
    #segments = text.split('\n')
    segments = nltk.sent_tokenize(text)
    
    for segment in segments:
        # Remove unwanted words and extra spaces
        #segment_words = [word.strip() for word in segment.split() if word not in remove_words and word not in stopwords and word in all_words]
        segment_words = [word.strip() for word in segment.split()if word in all_words]
        segment = ' '.join(segment_words)
        if 25 < len(segment) < 15000:
            final.append(segment)
            csv_out.append([year_month_day, doc_type, segment])
            keyword_freq_ts[year_month_day][doc_type].update(segment_words)
            global_word_counter.update(segment_words)
                    
print(len(final))

# Sort words by frequency
sorted_word_freq = sorted(global_word_counter.items(), key=lambda x: x[1], reverse=True)

# Write the processed data to a JSON file
write_data(f"{Model_Folder}/{Model}_texts.json", final)
write_data(f"{Model_Folder}/keyword_freq_ts_{Model}.json", keyword_freq_ts)

df_csv_out = pd.DataFrame(csv_out, columns=["date", "type", "segment"])
df_csv_out.to_csv(f"{Model_Folder}/{Model}_texts.csv", index=True)

# Write sorted words and their frequencies to a file
with open(f"{Model_Folder}/{Model}_word_freq.csv", "w", newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['Word', 'Frequency'])  # Column headers
    for word, freq in sorted_word_freq:
        writer.writerow([word, freq])
