{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kylenabors/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch \n",
    "import nltk\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()\n",
    "\n",
    "torch.backends.mps.is_built()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling\n",
      "/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling\n",
      "/Users/kylenabors/Documents\n",
      "/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Importing Configs\n",
    "# Define the path where config.py is located\n",
    "#Mac\n",
    "os.chdir('/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling')\n",
    "#Linux\n",
    "#os.chdir('/home/kwnabors/Documents/GitHub/Finance-ML-Modeling')\n",
    "config_file_path = os.getcwd()\n",
    "print(config_file_path)\n",
    "\n",
    "# Add this path to the sys.path\n",
    "sys.path.append(config_file_path)\n",
    "\n",
    "import config\n",
    "#Variables, Paramaters, and Pathnames needed for this script\n",
    "database_file = config.database\n",
    "database_folder = config.database_folder\n",
    "bert_models = config.bert_models\n",
    "bert_models_local = config.bert_models_local\n",
    "keywords = config.keywords\n",
    "finbert_models = config.finbert_models\n",
    "\n",
    "Body = config.Body\n",
    "Model = config.Model\n",
    "Model_Subfolder = f'/{Body} Texts/{Model}'\n",
    "Model_Folder = config.texts\n",
    "Model_Folder = Model_Folder + Model_Subfolder\n",
    "\n",
    "df = pd.read_csv(f\"{Model_Folder}/{Model}_texts.csv\") \n",
    "if Model == \"Beige Book\":\n",
    "    print(\"skip\")\n",
    "else:\n",
    "    df = df[df['language'] == 'en']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abaa91ee36d24fc292da197a6eb47d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fdf4cf20274f8dbeb49efc455aa4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca04703030614990ae1605a6b3dbf972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eeaa1d964184a22a5a9cd5208e54ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a32dffb0ee4ae5b8d132c85431ef1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f2049c26a94bf4aef1eaa61e7147aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be68d4e83f3541f0a6ef7a32520cd981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9379a25e18d440a78254ef371c0090e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46a2b9f7be2480fb84d906e7f503645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43dd3def7904adea2d28776ab237084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd178d4ec3f4ee98f9cbdc37d7ff379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14691c590d894a4280204643f95e3b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d17508d516b40c6bb2995d01a8da905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Finbert \n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", force_download=True)\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", force_download=True)\n",
    "model_1 = model_1.to('mps')\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',\n",
    "                                                        num_labels=3, force_download=True)\n",
    "finbert = finbert.to('mps')\n",
    "tokenizer_2 = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone', force_download=True)\n",
    "\n",
    "labels = {0:'positive', 1:'negative',2:'neutral'}\n",
    "labels2 = {0:'neutral', 1:'positive',2:'negative'}\n",
    "out_1= []\n",
    "out_2 = []\n",
    "sent_val = list()\n",
    "tone_val = list()\n",
    "long = 0\n",
    "errors = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fed_list = [\"Speeches\",\"Minutes\", \"Beige Book\", \"Statements\"]\n",
    "ECB_list = [\"Speeches\", \"Monetary policy decisions\", \"Economic Bulletin\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "skip\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for i in Fed_list:\n",
    "    out_1= []\n",
    "    out_2 = []\n",
    "    sent_val = list()\n",
    "    tone_val = list()\n",
    "    long = 0\n",
    "    errors = 0\n",
    "    total = 0\n",
    "    Body = \"Fed\"\n",
    "    Model = i\n",
    "    Model_Subfolder = f'/{Body} Texts/{Model}'\n",
    "    Model_Folder = config.texts\n",
    "    Model_Folder = Model_Folder + Model_Subfolder\n",
    "\n",
    "    df = pd.read_csv(f\"{Model_Folder}/{Model}_texts.csv\") \n",
    "    if Model == \"Beige Book\":\n",
    "        print(\"skip\")\n",
    "    else:\n",
    "        df = df[df['language'] == 'en']\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        docs = row[\"segment\"]\n",
    "        timestamps = row['date']\n",
    "        title = row['title']\n",
    "        docs = str(docs)\n",
    "        doc_num = row['doc_num']\n",
    "        event = row['event']\n",
    "        \n",
    "        total += 1\n",
    "        try:\n",
    "            # inputs_1 = tokenizer_1(docs, return_tensors=\"pt\", padding='max_length', max_length=511).to('mps')\n",
    "            # outputs_1 = model_1(**inputs_1)\n",
    "            # val_1 = torch.nn.functional.softmax(outputs_1.logits, dim=-1).to('cpu')\n",
    "            # val_1 = val_1.detach().numpy()  \n",
    "            \n",
    "            # positive = val_1[:, 0][0]\n",
    "            # negative = val_1[:, 1][0]\n",
    "            # neutral = val_1[:, 2][0]\n",
    "            # net = labels[np.argmax(val_1)]\n",
    "\n",
    "            # out_1.append([doc_num, timestamps, event, title, docs, positive, negative, neutral, net])\n",
    "            \n",
    "\n",
    "            inputs_2 = tokenizer_2(docs, return_tensors=\"pt\", padding='max_length', max_length=511).to('mps')\n",
    "            outputs_2 = finbert(**inputs_2)[0]\n",
    "            val_2 = labels2[np.argmax(outputs_2.to('cpu').detach().numpy())]\n",
    "            out_2.append([doc_num,  timestamps, event, title, docs, val_2])\n",
    "            \n",
    "        except:\n",
    "            errors += 1\n",
    "\n",
    "    # percent = (errors/total)*100\n",
    "    # print(f'Errors Long: {errors}')\n",
    "    # print(f'Errors Long %: {percent}')\n",
    "    # df_out_1 = pd.DataFrame(out_1, columns=[\"doc_num\", \"date\", \"event\", \"title\",  \"segment\", \"positive\", \"negative\", \"neutral\", \"sentiment\"])\n",
    "    # df_out_1[\"sentiment\"] = df_out_1[\"sentiment\"].replace({'positive': 1, 'neutral' : 0, 'negative' : -1})\n",
    "\n",
    "    # df_out_1.to_csv(f\"{finbert_models}/{Body}/{Model}/{Body}_{Model}_finbert_model_short.csv\")  \n",
    "    \n",
    "    df_out_2 = pd.DataFrame(out_2, columns=[\"doc_num\", \"date\", \"event\", \"title\", \"segment\", \"tone\"])\n",
    "    df_out_2.to_csv(f\"{finbert_models}/{Body}/{Model}/{Body}_{Model}_finbert_model_short_2.csv\") \n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for i in ECB_list:\n",
    "    out_1= []\n",
    "    out_2 = []\n",
    "    sent_val = list()\n",
    "    tone_val = list()\n",
    "    long = 0\n",
    "    errors = 0\n",
    "    total = 0\n",
    "    Body = \"ECB\"\n",
    "    Model = i\n",
    "    Model_Subfolder = f'/{Body} Texts/{Model}'\n",
    "    Model_Folder = config.texts\n",
    "    Model_Folder = Model_Folder + Model_Subfolder\n",
    "\n",
    "    df = pd.read_csv(f\"{Model_Folder}/{Model}_texts.csv\") \n",
    "    if Model == \"Beige Book\":\n",
    "        print(\"skip\")\n",
    "    else:\n",
    "        df = df[df['language'] == 'en']\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        docs = row[\"segment\"]\n",
    "        timestamps = row['date']\n",
    "        title = row['title']\n",
    "        docs = str(docs)\n",
    "        doc_num = row['doc_num']\n",
    "        event = row['event']\n",
    "        \n",
    "        total += 1\n",
    "        try:\n",
    "            # inputs_1 = tokenizer_1(docs, return_tensors=\"pt\", padding='max_length', max_length=511).to('mps')\n",
    "            # outputs_1 = model_1(**inputs_1)\n",
    "            # val_1 = torch.nn.functional.softmax(outputs_1.logits, dim=-1).to('cpu')\n",
    "            # val_1 = val_1.detach().numpy()  \n",
    "            \n",
    "            # positive = val_1[:, 0][0]\n",
    "            # negative = val_1[:, 1][0]\n",
    "            # neutral = val_1[:, 2][0]\n",
    "            # net = labels[np.argmax(val_1)]\n",
    "\n",
    "            # out_1.append([doc_num, timestamps, event, title, docs, positive, negative, neutral, net])\n",
    "            \n",
    "            inputs_2 = tokenizer_2(docs, return_tensors=\"pt\", padding='max_length', max_length=511).to('mps')\n",
    "            outputs_2 = finbert(**inputs_2)[0]\n",
    "            val_2 = labels2[np.argmax(outputs_2.to('cpu').detach().numpy())]\n",
    "            out_2.append([doc_num,  timestamps, event, title, docs, val_2])\n",
    "            \n",
    "        except:\n",
    "            errors += 1\n",
    "\n",
    "    # percent = (errors/total)*100\n",
    "    # print(f'Errors Long: {errors}')\n",
    "    # print(f'Errors Long %: {percent}')\n",
    "    # df_out_1 = pd.DataFrame(out_1, columns=[\"doc_num\", \"date\", \"event\", \"title\", \"segment\", \"positive\", \"negative\", \"neutral\", \"sentiment\"])\n",
    "    # df_out_1[\"sentiment\"] = df_out_1[\"sentiment\"].replace({'positive': 1, 'neutral' : 0, 'negative' : -1})\n",
    "\n",
    "    # df_out_1.to_csv(f\"{finbert_models}/{Body}/{Model}/{Body}_{Model}_finbert_model_short.csv\")  \n",
    "    \n",
    "    df_out_2 = pd.DataFrame(out_2, columns=[\"doc_num\", \"date\", \"event\", \"title\", \"segment\", \"tone\"])\n",
    "    df_out_2.to_csv(f\"{finbert_models}/{Body}/{Model}/{Body}_{Model}_finbert_model_short_2.csv\") \n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Totally Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
