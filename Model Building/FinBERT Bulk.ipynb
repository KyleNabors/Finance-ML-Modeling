{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    Trainer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "platform.platform()\n",
    "\n",
    "torch.backends.mps.is_built()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    # x = torch.ones(1, device=mps_device)\n",
    "    # print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def remove_comma(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    indices = []\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.dep_ == \"punct\":\n",
    "            try:\n",
    "                next_token = doc[i + 1]\n",
    "                if next_token.dep_ == \"ROOT\" or next_token.dep_ == \"conj\":\n",
    "                    indices.append(i)\n",
    "            except IndexError:\n",
    "                pass\n",
    "    if not indices:\n",
    "        return sentence\n",
    "    else:\n",
    "        parts = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            parts.append(doc[last_idx:idx].text.strip())\n",
    "\n",
    "            last_idx = idx + 1\n",
    "        parts.append(doc[last_idx:].text.strip())\n",
    "        return \" \".join(parts)\n",
    "\n",
    "\n",
    "def sentiment_focus(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    focus = \"\"\n",
    "    focus_changed = 1\n",
    "    for token in doc[:-1]:\n",
    "        if token.lower_ == \"but\":\n",
    "            focus = doc[token.i + 1 :]\n",
    "            return str(focus).strip(), focus_changed\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_tokens = [token for token in sent]\n",
    "        for token in sent_tokens:\n",
    "            if token.lower_ == \"although\" or token.lower_ == \"though\":\n",
    "                try:\n",
    "                    comma_index_back = [\n",
    "                        token1.i for token1 in doc[token.i :] if token1.text == \",\"\n",
    "                    ][0]\n",
    "                except IndexError:\n",
    "                    try:\n",
    "                        comma_index_front = [\n",
    "                            token1.i for token1 in doc[: token.i] if token1.text == \",\"\n",
    "                        ][-1]\n",
    "                    except IndexError:\n",
    "                        return str(doc).strip(), focus_changed\n",
    "                    focus = doc[:comma_index_front].text\n",
    "                    return str(focus).strip(), focus_changed\n",
    "                try:\n",
    "                    comma_index_front = [\n",
    "                        token1.i for token1 in doc[: token.i] if token1.text == \",\"\n",
    "                    ][-1]\n",
    "                except IndexError:\n",
    "                    focus = doc[comma_index_back + 1 :].text\n",
    "                    return str(focus).strip(), focus_changed\n",
    "                focus = doc[:comma_index_front].text + doc[comma_index_back:].text\n",
    "                return str(focus).strip(), focus_changed\n",
    "\n",
    "    if doc[0].lower_ == \"while\":\n",
    "        try:\n",
    "            comma_index_back1 = [token2.i for token2 in doc if token2.text == \",\"][0]\n",
    "        except IndexError:\n",
    "            return str(doc).strip(), focus_changed\n",
    "        focus = doc[comma_index_back1 + 1 :].text\n",
    "        return str(focus).strip(), focus_changed\n",
    "\n",
    "    focus_changed = 0\n",
    "    return str(doc).strip(), focus_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling\n",
      "/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling\n",
      "/Users/kylenabors/Documents\n",
      "/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling\n"
     ]
    }
   ],
   "source": [
    "# Importing Configs\n",
    "# Define the path where config.py is located\n",
    "# Mac\n",
    "os.chdir(\"/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling\")\n",
    "# Linux\n",
    "# os.chdir('/home/kwnabors/Documents/GitHub/Finance-ML-Modeling')\n",
    "config_file_path = os.getcwd()\n",
    "print(config_file_path)\n",
    "\n",
    "# Add this path to the sys.path\n",
    "sys.path.append(config_file_path)\n",
    "\n",
    "import config\n",
    "\n",
    "# Variables, Paramaters, and Pathnames needed for this script\n",
    "database_file = config.database\n",
    "database_folder = config.database_folder\n",
    "bert_models = config.bert_models\n",
    "bert_models_local = config.bert_models_local\n",
    "keywords = config.keywords\n",
    "finbert_models = config.finbert_models\n",
    "\n",
    "Body = config.Body\n",
    "Model = config.Model\n",
    "Model_Subfolder = f\"/{Body} Texts/{Model}\"\n",
    "Model_Folder = config.texts\n",
    "Model_Folder = Model_Folder + Model_Subfolder\n",
    "\n",
    "df = pd.read_csv(f\"{Model_Folder}/{Model}_texts.csv\")\n",
    "if Model == \"Beige Book\":\n",
    "    print(\"skip\")\n",
    "else:\n",
    "    df = df[df[\"language\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_1 = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", force_download=True)\n",
    "# model_1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"ProsusAI/finbert\", force_download=True\n",
    "# )\n",
    "# model_1 = model_1.to(\"mps\")\n",
    "\n",
    "model_2 = BertForSequenceClassification.from_pretrained(\n",
    "    \"ZiweiChen/FinBERT-FOMC\", num_labels=3\n",
    ").to(\"mps\")\n",
    "tokenizer_2 = BertTokenizer.from_pretrained(\"ZiweiChen/FinBERT-FOMC\")\n",
    "finbert_fomc = pipeline(\n",
    "    \"text-classification\", model=model_2, tokenizer=tokenizer_2, device=\"mps\"\n",
    ")\n",
    "\n",
    "\n",
    "labels = {0: \"positive\", 1: \"negative\", 2: \"neutral\"}\n",
    "labels2 = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
    "out_1 = []\n",
    "out_2 = []\n",
    "sent_val = list()\n",
    "tone_val = list()\n",
    "long = 0\n",
    "errors = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fed_list = [\n",
    "    # \"Speeches\",\n",
    "    # \"Minutes\",\n",
    "    # \"Beige Book\",\n",
    "    # \"Statements\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for i in Fed_list:\n",
    "    out_1 = []\n",
    "    out_2 = []\n",
    "    sent_val = list()\n",
    "    tone_val = list()\n",
    "    long = 0\n",
    "    errors = 0\n",
    "    total = 0\n",
    "    Body = \"Fed\"\n",
    "    Model = i\n",
    "    Model_Subfolder = f\"/{Body} Texts/{Model}\"\n",
    "    Model_Folder = config.texts\n",
    "    Model_Folder = Model_Folder + Model_Subfolder\n",
    "\n",
    "    df = pd.read_csv(f\"{Model_Folder}/{Model}_texts.csv\")\n",
    "    if Model == \"Beige Book\":\n",
    "        print(\"skip\")\n",
    "    else:\n",
    "        df = df[df[\"language\"] == \"en\"]\n",
    "\n",
    "    df = df[(df[\"date\"] >= \"1999-01-01\") & (df[\"date\"] <= \"2024-06-30\")]\n",
    "    tqdm.pandas()\n",
    "\n",
    "    df[\"sentence_simple\"] = df[\"segment\"].progress_apply(remove_comma)\n",
    "    # Processing sentiment focus\n",
    "    df[[\"sentence_simple\", \"focus_changed\"]] = (\n",
    "        df[\"sentence_simple\"].progress_apply(sentiment_focus).apply(pd.Series)\n",
    "    )\n",
    "\n",
    "    # df[\"focus_ornot\"] = df[\"focus_changed\"].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "    df.drop(\"focus_changed\", axis=1, inplace=True)\n",
    "\n",
    "    df[\"len\"] = df[\"sentence_simple\"].apply(lambda x: len(x))\n",
    "    df = df[df[\"len\"] < 512]\n",
    "\n",
    "    df[\"sentiment\"] = df[\"sentence_simple\"].progress_apply(lambda x: finbert_fomc(x))\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].apply(lambda x: x[0][\"label\"])\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].replace(\n",
    "        {\"Positive\": 1, \"Neutral\": 0, \"Negative\": -1}\n",
    "    )\n",
    "    print(df[\"sentiment\"].unique())\n",
    "\n",
    "    df = df[[\"date\", \"segment\", \"sentiment\", \"type\"]]\n",
    "    df.to_csv(f\"{finbert_models}/{Body}/{Model}/{Body}_{Model}_finbert_model_short.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECB_list = [\n",
    "    # \"Speeches\",\n",
    "    # \"Monetary policy decisions\",\n",
    "    \"Economic Bulletin\",\n",
    "    # \"Press Conferences\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed181c8a4c142cd93f0f2083ed3c8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58bfe7649864db999bd4ccafed635ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c1447394434aa9a68ca1a0c7fc2f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 -1  1]\n"
     ]
    }
   ],
   "source": [
    "for i in ECB_list:\n",
    "    out_1 = []\n",
    "    out_2 = []\n",
    "    sent_val = list()\n",
    "    tone_val = list()\n",
    "    long = 0\n",
    "    errors = 0\n",
    "    total = 0\n",
    "    Body = \"ECB\"\n",
    "    Model = i\n",
    "    Model_Subfolder = f\"/{Body} Texts/{Model}\"\n",
    "    Model_Folder = config.texts\n",
    "    Model_Folder = Model_Folder + Model_Subfolder\n",
    "\n",
    "    df = pd.read_csv(f\"{Model_Folder}/{Model}_texts.csv\")\n",
    "    if Model == \"Beige Book\":\n",
    "        print(\"skip\")\n",
    "    else:\n",
    "        df = df[df[\"language\"] == \"en\"]\n",
    "\n",
    "    df = df[(df[\"date\"] >= \"1999-01-01\") & (df[\"date\"] <= \"2024-06-30\")]\n",
    "    tqdm.pandas()\n",
    "\n",
    "    df[\"sentence_simple\"] = df[\"segment\"].progress_apply(remove_comma)\n",
    "    # Processing sentiment focus\n",
    "    df[[\"sentence_simple\", \"focus_changed\"]] = (\n",
    "        df[\"sentence_simple\"].progress_apply(sentiment_focus).apply(pd.Series)\n",
    "    )\n",
    "\n",
    "    # df[\"focus_ornot\"] = df[\"focus_changed\"].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "    df.drop(\"focus_changed\", axis=1, inplace=True)\n",
    "\n",
    "    df[\"len\"] = df[\"sentence_simple\"].apply(lambda x: len(x))\n",
    "    df = df[df[\"len\"] < 512]\n",
    "\n",
    "    df[\"sentiment\"] = df[\"sentence_simple\"].progress_apply(lambda x: finbert_fomc(x))\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].apply(lambda x: x[0][\"label\"])\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].replace(\n",
    "        {\"Positive\": 1, \"Neutral\": 0, \"Negative\": -1}\n",
    "    )\n",
    "    print(df[\"sentiment\"].unique())\n",
    "    df = df[[\"date\", \"segment\", \"sentiment\", \"type\"]]\n",
    "    df.to_csv(f\"{finbert_models}/{Body}/{Model}/{Body}_{Model}_finbert_model_short.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Totally Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
