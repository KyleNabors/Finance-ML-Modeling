# Import necessary libraries
import sys
import os
import json
import pdfplumber
import csv
from collections import defaultdict, Counter
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec
import spacy
import csv
import nltk
from nltk.corpus import words
import pandas as pd

import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel


nlp = spacy.load("en_core_web_lg")

nltk.download('words')
hyphenated_words = set(word for word in words.words() if '-' in word)

# Define the path where config.py is located
config_path = '/Users/kylenabors/Documents/GitHub/Finance-ML-Modeling'

# Add this path to the sys.path
sys.path.append(config_path)

# Now Python knows where to find config.py
import config

#Variables, Paramaters, and Pathnames needed for this script
database_file = config.database
keyword_freq_ts = config.keyword_freq_ts
database_folder = config.database_folder
scale = config.scale
models = config.models 
print(scale)

# Load data from the JSON database
def load_data(file):
    with open(file, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

# Write data to a JSON file
def write_data(file, data):
    with open(file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)
        
#Load Text Segments
df = pd.read_csv(f"{database_folder}/Fed Data/fed_data_blocks.csv")
df.drop(columns=["Unnamed: 0"], inplace=True)
print(df.head())

keyword_freq_ts = load_data(keyword_freq_ts)

texts = df["segment"].tolist()
dates = df["date"].tolist()

#Remove Stopwords
def lemmatization(data, allowed_postags=["NOUN", "ADJ", "VERB", "ADV"]):
    nlp = spacy.load("en_core_web_lg", disable=["parser", "ner"])
    texts_out = []
    for text in texts:  # unpack the sublist into text and label
        doc = nlp(text)
        new_text = []
        for token in doc:
            if token.pos_ in allowed_postags:
                new_text.append(token.lemma_)
        final = " ".join(new_text)
        texts_out.append(final)
    return (texts_out)

#Lemmatize
lemmatized_texts = lemmatization(texts)

#Remove Stopwords
def gen_words(texts):
    final = []
    for text in texts:
        new = gensim.utils.simple_preprocess(text, deacc=True)
        final.append(new)
    return(final)

#Create Dictionary
data_words = gen_words(lemmatized_texts)

#Bigrams and Trigrams
bigrams_phrases = gensim.models.Phrases(data_words, min_count=10*scale, threshold=(15*scale))
trigrams_phrases = gensim.models.Phrases(bigrams_phrases[data_words], threshold=15*scale)

bigrams = gensim.models.phrases.Phraser(bigrams_phrases)
trigram = gensim.models.phrases.Phraser(trigrams_phrases)

def make_bigrams(texts):
    return([bigrams[doc] for doc in data_words])

def make_trigram(texts):
    return([trigram[bigrams[doc]] for doc in data_words])

data_bigrams = make_bigrams(data_words)
data_bigrams_trigrams = make_trigram(data_bigrams)

#TF-IDF REMOVAL
from gensim.models import TfidfModel

id2word = corpora.Dictionary(data_bigrams_trigrams)
texts = data_bigrams_trigrams
corpus = [id2word.doc2bow(text) for text in data_words]
tfidf = TfidfModel(corpus, id2word=id2word)

low_value = 0.03
words  = []
words_missing_in_tfidf = []
for i in range(0, len(corpus)):
    bow = corpus[i]
    low_value_words = [] #reinitialize to be safe. You can skip this.
    tfidf_ids = [id for id, value in tfidf[bow]]
    bow_ids = [id for id, value in bow]
    low_value_words = [id for id, value in tfidf[bow] if value < low_value]
    drops = low_value_words+words_missing_in_tfidf
    for item in drops:
        words.append(id2word[item])
    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing

    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]
    corpus[i] = new_bow
    
#Build Word2Vec Model

model = Word2Vec(texts, min_count=25, workers=10, window=5, sg=1, epochs= 20)
keywords = config.keywords

# Find the top 10 words similar to a given list of keywords
keywords = config.keywords
results = []
for keyword in keywords:
    res = model.wv.similar_by_word(keyword, topn=10)
    for item in res:
        results.append([keyword] + list(item))
        
# Save the results in a CSV file
with open(f'{models}/Word2Vec Models/similar_words.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Keyword", "Similar Word", "Similarity"])
    writer.writerows(results)

# List to hold training data
train_data = []
keyword_dict = {keyword: i+1 for i, keyword in enumerate(keywords)}
keyword_freq = Counter()

# Filter the processed data based on search words
for segment in texts:
    for word in keywords:
        if word in segment:
            segment = " ".join(segment)
            train_data.append((segment, keyword_dict[word]))
            keyword_freq[word] += 1

# Write the training data to a JSON file
write_data(f'{models}/Word2Vec Models/fed_data_train.json', train_data)

# Write keyword information to a CSV file
with open(f'{models}/Word2Vec Models/keyword_info.csv', "w", newline="") as csvfile:
    fieldnames = ['Keyword', 'Number', 'Frequency']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for keyword, number in keyword_dict.items():
        writer.writerow({'Keyword': keyword, 'Number': number, 'Frequency': keyword_freq[keyword]})

# Create a set of all year-month values
year_month_day_set = set(keyword_freq_ts.keys())

# Create a set of all document types
doc_types_list = set()
for year_month, doc_types in keyword_freq_ts.items():
    for doc_type in doc_types.keys():
        doc_types_list.add(doc_type)

with open(f'{models}/Word2Vec Models/keyword_info_ts.csv', "w", newline="") as csvfile:
    fieldnames = ['Year-Month-Day', 'Type', 'Keyword', 'Frequency']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    
    for year_month_day in year_month_day_set:
        for doc_type in doc_types_list:
            for keyword in keywords:
                # Set default frequency to 0
                freq = 0
                
                # If the year-month-day and keyword exist in the data, update the frequency
                if year_month_day in keyword_freq_ts and doc_type in keyword_freq_ts[year_month_day] and keyword in keyword_freq_ts[year_month_day][doc_type]:
                    freq = keyword_freq_ts[year_month_day][doc_type][keyword]
                
                writer.writerow({'Year-Month-Day': year_month_day, 'Type': doc_type, 'Keyword': keyword, 'Frequency': freq})





exit()





results = []

keywords = config.keywords
results = []
for keyword in keywords:
    res = model.wv.similar_by_word(keyword, topn=10)
    for item in res:
        results.append([keyword] + list(item))





for keyword in keywords:
    for date in dates:
        res = model.wv.similar_by_word(keyword, topn=10)
        for item in res:
            results.append([date] + [keyword] + list(item))
        
similar_words = pd.DataFrame(results, columns=["date", "keyword", "similar_word", "similarity"])
print(similar_words.head(100))

