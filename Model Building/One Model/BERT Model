import os 
import sys
import json
import pandas as pd
import numpy as np

from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from hdbscan import HDBSCAN
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration
from bertopic.vectorizers import ClassTfidfTransformer

from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
import torch 
import nltk
import spacy

import pygwalker as pyg
from datetime import datetime

nlp = spacy.load("en_core_web_lg")

nltk.download('stopwords') 

# NLTK English stopwords
stopwords = nltk.corpus.stopwords.words('english')


#Find and import config file
config_path = os.getcwd()
sys.path.append(config_path)
import config


#Variables, Paramaters, and Pathnames needed for this script
database_file = config.database
database_folder = config.database_folder
bert_models = config.bert_models
bert_models_local = config.bert_models_local



Body = 'Fed'
Model = 'Monetary Policy Report'
Model_Subfolder = f'/{Body} Texts/{Model}'
Model_Folder = config.texts
Model_Folder = Model_Folder + Model_Subfolder
print(Model_Folder)

#-------------------------------------------------------------------------------------------------------------------
# df = pd.read_csv(f"{Model_Folder}/{Model}_texts.csv")  
# docs = df["segment"].to_list()
# timestamps = df['date'].to_list()
# type = df['type'].to_list()

#-------------------------------------------------------------------------------------------------------------------
# df = pd.read_csv("/Users/kylenabors/Documents/MS-Thesis Data/Database/Fed Data/fed_data_blocks.csv")
# docs = df["segment"].to_list()
# timestamps = df['date'].to_list()
# type = df['type'].to_list()

#-------------------------------------------------------------------------------------------------------------------
#df = pd.read_csv("/Users/kylenabors/Documents/Database/Training Data/News/Finance News.csv")
# print(df.head())
# docs = df["description"].to_list()
# timestamps_raw = df['created_at'].to_list()
# type = df['main_domain'].to_list()

# timestamps = []
# for timestamp in timestamps_raw:
#     timestamp = timestamp[:10]
#     timestamps.append(timestamp)

#-------------------------------------------------------------------------------------------------------------------
# df = pd.read_csv("/Users/kylenabors/Documents/Database/Training Data/Fed/Press Data/Fed_Scrape-2015-2023.csv")
# docs = df["Text"].to_list()
# timestamps_raw = df['Date'].to_list()
# type = df['Type'].to_list()

# timestamps = []
# for timestamp in timestamps_raw:
#     timestamp = str(timestamp)
#     timestamp = datetime.strptime(timestamp, '%Y%m%d').strftime('%Y-%m-%d')
#     timestamps.append(timestamp)

#-------------------------------------------------------------------------------------------------------------------
df = pd.read_csv("/Users/kylenabors/Documents/Database/Training Data/Fed/Press Confrences/press_conference_transcript.csv")

# Specify the year and month you want to start and end processing files from
start_year_month_day = '2006-12-31'
end_year_month_day = '2023-12-31'
df = df[df['meeting_date'] >= start_year_month_day]
df = df[df['meeting_date'] <= end_year_month_day]

docs = df["text"].to_list()
timestamps = df['meeting_date'].to_list()
type = df['document_kind'].to_list()

#-------------------------------------------------------------------------------------------------------------------

topic_list = [
    ['interest', 'rate', 'target', 'increase'],
    ['credit'],
    ['inflation'],
    ['unemployment'],
    ['market'],
    ['trade'],
    ['energy'],
]

keywords = config.keywords

# Embedding
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(docs, batch_size=10, show_progress_bar=True)

#Reduce Dimensionality
umap_model = UMAP(n_neighbors=5, 
                  n_components=2, 
                  metric='cosine', 
                  n_epochs=500,
                  min_dist=0.0, 
                  target_metric_kwds=keywords, 
                  target_weight=0.95, 
                  verbose=True)

# Clustering model
cluster_model = HDBSCAN(min_cluster_size = 15, 
                        min_samples=10,
                        metric = 'euclidean', 
                        cluster_selection_method = 'eom', 
                        prediction_data = True)

#Representation model
representation_model = MaximalMarginalRelevance(diversity=0.2)

#Create UMAP model
vectorizer_model = CountVectorizer(stop_words=stopwords, ngram_range=(1, 1))

ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)

print("Done Preprocessing Data")
# BERTopic model
topic_model = BERTopic(language= 'english',
                       min_topic_size=15,
                       n_gram_range=(1, 1),
                       nr_topics = 64,
                       #seed_topic_list=topic_list,
                       embedding_model=embedding_model,
                       umap_model=umap_model,
                       hdbscan_model=cluster_model,
                       vectorizer_model=vectorizer_model,
                       ctfidf_model=ctfidf_model,
                       representation_model=representation_model,
                       verbose=True
                       ).fit(docs, embeddings = embeddings)

print("Done Creating BERTopic Model")

topics_per_class = topic_model.topics_per_class(docs, classes = type)

# Save topic-terms barcharts as HTML file
topic_model.visualize_barchart(top_n_topics = 32, n_words=8).write_html(f"{bert_models}/barchart.html")

torch.save(topic_model, f"{bert_models_local}/topic_model_{Model}.pt")

similar_topics, similarity = topic_model.find_topics("interest", top_n=10)
print(topic_model.get_topic(similar_topics[0]))
