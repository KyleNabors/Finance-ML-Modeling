

import os 
import sys
import pandas as pd
import numpy as np
from umap import UMAP
import nltk
import spacy
from nltk.tokenize import word_tokenize
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
import matplotlib.pyplot as plt
import random
from nltk.classify.scikitlearn import SklearnClassifier
import pickle
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from nltk.classify import ClassifierI
from statistics import mode
from sklearn.metrics import f1_score, accuracy_score
from sklearn.metrics import confusion_matrix

# NLTK English stopwords
nlp = spacy.load("en_core_web_lg")
nltk.download('stopwords') 
stopwords_en = nltk.corpus.stopwords.words('english')
stopwords_sp = nltk.corpus.stopwords.words('spanish')
stopwords_fr = nltk.corpus.stopwords.words('french')
stopwords_it = nltk.corpus.stopwords.words('italian')
stopwords = stopwords_en + stopwords_sp + stopwords_fr + stopwords_it

#Find and import config file
config_path = os.getcwd()
sys.path.append(config_path)
import config

#Variables, Paramaters, and Pathnames needed for this script
database_file = config.database
database_folder = config.database_folder
bert_models = config.bert_models
bert_models_local = config.bert_models_local
keywords = config.keywords
Sentiment_models = config.Sentiment_models

Body = config.Body
Model = config.Model
Model_Subfolder = f'/{Body} Texts/{Model}'
Model_Folder = config.texts
Model_Folder = Model_Folder + Model_Subfolder

df = pd.read_csv(f"{Model_Folder}/{Model}_texts.csv")  

Body_2 = config.Body_2
Model_2 = config.Model_2
Model_Subfolder_2 = f'/{Body_2} Texts/{Model_2}'
Model_Folder_2 = config.texts
Model_Folder_2 = Model_Folder_2 + Model_Subfolder_2

df_2 = pd.read_csv(f"{Model_Folder_2}/{Model_2}_texts.csv")  

df_all = pd.concat([df, df_2], axis=0)
#df_all = df_all.drop(columns=['Unnamed: 0'], inplace=True)
out = []
print(df.head())
print(df_2.head())
print(df_all)
sia = SentimentIntensityAnalyzer()

# #VADER Analysis 
for index, row in df_all.iterrows():
    docs = row["segment"]
    timestamps = row['date']
    type = row['type']
    title = row['title']
    docs = str(docs)
    
    good = sia.polarity_scores(docs)['pos']
    bad = sia.polarity_scores(docs)['neg']
    neutral = sia.polarity_scores(docs)['neu']
    compound = sia.polarity_scores(docs)['compound']
    out.append([timestamps, title, type, docs, good, bad, neutral, compound])
    
df_out = pd.DataFrame(out, columns=["date", "title", "type", "segment","good", "bad", "neutral", "compound"])

df_out.to_csv(f"{Model_Folder}/{Model}_vader_sentiment_texts.csv")  
print(df_out.head())

#Adding Custom Sentiment Analysis 
increase_words = ['increase', 'raise', 'increasing', 'raising', 'higher', 'hike', 'hiking', 'increases', 'raises', 'hikes']
decrease_words = ['decrease', 'lower', 'decline', 'declining', 'cut', 'cutting', 'reducing', 'reduction', 'reduce']
funds_words = ['interest', 'rate', 'funds', 'federal funds', 'points']


pos_train = []
neg_train = []
neutral_data = []
other_data = []

for index, row in df_all.iterrows():

    docs = row["segment"]
    title = row["title"]
    timestamps = row['date']
    type = row['type']
    docs = str(docs)

    interest = 0
    increase = 0
    decrease = 0
    good = 0
    bad = 0
    neutral = 0
    
    for word in docs.split():
        word = word.casefold()
        if word in funds_words:
            interest = 1
            
        if word in increase_words:
            increase = 1
 
        if word in decrease_words:
            decrease = 1
        
        if interest == 1 and increase == 1:
            bad = 1
            
        if interest == 1 and decrease == 1:
            good = 1  
        
        if interest == 1 and increase == 0 and decrease == 0:
            neutral = 1
    
    if good == 1:
        pos_train.append([timestamps, title, type, docs, interest, increase, decrease, good, bad, neutral])
        
    if bad == 1:
        neg_train.append([timestamps, title, type, docs, interest, increase, decrease, good, bad, neutral])
        
    if neutral == 1:
        neutral_data.append([timestamps, title, type, docs, interest, increase, decrease, good, bad, neutral])
        
    if good == 0 and bad == 0:
        other_data.append([timestamps, title, type, docs, interest, increase, decrease, good, bad, neutral])
    
    out.append([timestamps, type, docs, title, interest, increase, decrease, good, bad, neutral])


print(len(pos_train))
print(len(neg_train))

all_words = []
documents = []

#  j is adject, r is adverb, and v is verb
allowed_word_types = ["J","R","V"]
#allowed_word_types = ["J"]

# pos_train = pd.DataFrame(pos_train, columns=["date", "type", title, "segment", "interest", "increase", "decrease", "good", "bad", "neutral"])
# for index, row in pos_train.iterrows():
#     docs = row["segment"]
#     timestamps = row['date']
#     type = row['type']
#     docs = str(docs)

#     documents.append( (docs, "pos") )

#     #Tokenize 
#     tokenized = word_tokenize(docs)

#     #Remove Stopwords 
#     cleaned = [w for w in tokenized if not w in stopwords]

#     # parts of speech tagging for each word 
#     pos = nltk.pos_tag(cleaned)

#     # make a list of  all adjectives identified by the allowed word types list above
#     for w in pos:
#         if w[1][0] in allowed_word_types:
#             all_words.append(w[0].lower())
    
    
# neg_train = pd.DataFrame(neg_train, columns=["date", title, "type", "segment", "interest", "increase", "decrease", "good", "bad", "neutral"])

# for index, row in neg_train.iterrows():

#     docs = row["segment"]
#     timestamps = row['date']
#     type = row['type']
#     docs = str(docs)

#     documents.append( (docs, "neg") )
    
#     #Tokenize
#     tokenized = word_tokenize(docs)
    
#     #Remove Stopwords
#     cleaned = [w for w in tokenized if not w in stopwords]
    
#     # parts of speech tagging for each word 
#     neg = nltk.pos_tag(cleaned)
    
#     #Parts of the Speech Tagging for each word 
#     for w in neg:
#         if w[1][0] in allowed_word_types:
#             all_words.append(w[0].lower())
    

for index, row in df_out.iterrows():  
    good = row["good"]
    bad = row["bad"]
    if good > 0.5:
        docs = row["segment"]
        title = row["title"]
        timestamps = row['date']
        type = row['type']
        docs = str(docs)
        
        documents.append( (docs, "pos") )
        
        #Tokenize 
        tokenized = word_tokenize(docs)

        #Remove Stopwords 
        cleaned = [w for w in tokenized if not w in stopwords]
        
        # parts of speech tagging for each word 
        pos = nltk.pos_tag(cleaned)
        
        # make a list of  all adjectives identified by the allowed word types list above
        for w in pos:
            if w[1][0] in allowed_word_types:
                all_words.append(w[0].lower())
    

for index, row in df_out.iterrows():

    bad = row["bad"]
    if bad > 0.5:
        docs = row["segment"]
        title = row["title"]
        timestamps = row['date']
        type = row['type']
        docs = str(docs)

        documents.append( (docs, "neg") )
        
        #Tokenize
        tokenized = word_tokenize(docs)
        
        #Remove Stopwords
        cleaned = [w for w in tokenized if not w in stopwords]
        
        # parts of speech tagging for each word 
        neg = nltk.pos_tag(cleaned)
        
        #Parts of the Speech Tagging for each word 
        for w in neg:
            if w[1][0] in allowed_word_types:
                all_words.append(w[0].lower())

print(len(all_words))

pos_A = []
for w in pos:
    if w[1][0] in allowed_word_types:
        pos_A.append(w[0].lower())
        
pos_N = []
for w in neg:
    if w[1][0] in allowed_word_types:
        pos_N.append(w[0].lower())


# from wordcloud import WordCloud
# text = ' '.join(pos_A)
# wordcloud = WordCloud().generate(text)

# plt.figure(figsize = (15, 9))
# # Display the generated image:
# plt.imshow(wordcloud, interpolation= "bilinear")
# plt.axis("off")
# plt.show()

# pickling the list documents to save future recalculations 
save_documents = open(f'{Sentiment_models}/pickled_algos/documents.pickle', "wb")
pickle.dump(documents, save_documents)
save_documents.close()

BOW = nltk.FreqDist(all_words)
BOW

word_features = list(BOW.keys())[:5000]
word_features[0], word_features[-1]

save_word_features = open(f'{Sentiment_models}/pickled_algos/word_features5k.pickle', "wb")
pickle.dump(word_features, save_word_features)
save_word_features.close()

def find_features(document):
    words = word_tokenize(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)
    return features

# Creating features for each review
featuresets = [(find_features(doc), category) for (doc, category) in documents]
fs_scale = len(featuresets)

# Shuffling the documents 
random.shuffle(featuresets)

ts_var = fs_scale * 0.85
ts_var = int(ts_var)

training_set = featuresets[:ts_var]
testing_set = featuresets[ts_var:]

classifier = nltk.NaiveBayesClassifier.train(training_set)

accuracy = nltk.classify.accuracy(classifier, testing_set) * 100
accuracy = str(round(accuracy, 2))
print(f"Baysian Machine Learning Classifier Accuracy:{accuracy}%")

classifier.show_most_informative_features(15)

mif = classifier.most_informative_features()
mif = [a for a,b in mif]
print(mif)

ground_truth = [r[1] for r in testing_set]

preds = [classifier.classify(r[0]) for r in testing_set]

f1_score(ground_truth, preds, labels = ['neg', 'pos'], average = 'micro')

y_test = ground_truth
y_pred = preds
class_names = ['neg', 'pos']

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):

    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

# training various models by passing in the sklearn models into the SklearnClassifier from NLTK 

MNB_clf = SklearnClassifier(MultinomialNB())
MNB_clf.train(training_set)
print("MNB_classifier accuracy percent:", (nltk.classify.accuracy(MNB_clf, testing_set))*100)

BNB_clf = SklearnClassifier(BernoulliNB())
BNB_clf.train(training_set)
print("BernoulliNB_classifier accuracy percent:", (nltk.classify.accuracy(BNB_clf, testing_set))*100)

LogReg_clf = SklearnClassifier(LogisticRegression())
LogReg_clf.train(training_set)
print("LogisticRegression_classifier accuracy percent:", (nltk.classify.accuracy(LogReg_clf, testing_set))*100)

SGD_clf = SklearnClassifier(SGDClassifier())
SGD_clf.train(training_set)
print("SGDClassifier_classifier accuracy percent:", (nltk.classify.accuracy(SGD_clf, testing_set))*100)

SVC_clf = SklearnClassifier(SVC())
SVC_clf.train(training_set)
print("SVC_classifier accuracy percent:", (nltk.classify.accuracy(SVC_clf, testing_set))*100)

def create_pickle(c, file_name): 
    save_classifier = open(file_name, 'wb')
    pickle.dump(c, save_classifier)
    save_classifier.close()

classifiers_dict = {'ONB': [classifier, f'{Sentiment_models}/pickled_algos/ONB_clf.pickle'],
                    'MNB': [MNB_clf, f'{Sentiment_models}/pickled_algos/MNB_clf.pickle'],
                    'BNB': [BNB_clf, f'{Sentiment_models}/pickled_algos/BNB_clf.pickle'],
                    'LogReg': [LogReg_clf, f'{Sentiment_models}/pickled_algos/LogReg_clf.pickle'],
                    'SGD': [SGD_clf, f'{Sentiment_models}/pickled_algos/SGD_clf.pickle'], 
                    'SVC': [SVC_clf, f'{Sentiment_models}/pickled_algos/SVC_clf.pickle']}

for clf, listy in classifiers_dict.items(): 
    create_pickle(listy[0], listy[1])


ground_truth = [r[1] for r in testing_set]
predictions = {}
f1_scores = {}
acc_scores = {}

for clf, listy in classifiers_dict.items(): 
    predictions[clf] = [listy[0].classify(r[0]) for r in testing_set]
    f1_scores[clf] = f1_score(ground_truth, predictions[clf], labels = ['neg', 'pos'], average = 'micro')
    print(f'f1_score {clf}: {f1_scores[clf]}')
    
for clf, listy in classifiers_dict.items(): 
    acc_scores[clf] = accuracy_score(ground_truth, predictions[clf])
    print(f'Accuracy_score {clf}: {acc_scores[clf]}')
    
from nltk.classify import ClassifierI

# Defininig the ensemble model class 
class EnsembleClassifier(ClassifierI):
    
    def __init__(self, *classifiers):
        self._classifiers = classifiers
    
    # returns the classification based on majority of votes
    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)
    # a simple measurement the degree of confidence in the classification 
    def confidence(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)

        choice_votes = votes.count(mode(votes))
        conf = choice_votes / len(votes)
        return conf
# function to load models given filepath
def load_model(file_path): 
    classifier_f = open(file_path, "rb")
    classifier = pickle.load(classifier_f)
    classifier_f.close()
    return classifier

# Original Naive Bayes Classifier
ONB_Clf = load_model(f'{Sentiment_models}/pickled_algos/ONB_clf.pickle')

# Multinomial Naive Bayes Classifier 
MNB_Clf = load_model(f'{Sentiment_models}/pickled_algos/MNB_clf.pickle')

# Bernoulli  Naive Bayes Classifier 
BNB_Clf = load_model(f'{Sentiment_models}/pickled_algos/BNB_clf.pickle')

# Logistic Regression Classifier 
LogReg_Clf = load_model(f'{Sentiment_models}/pickled_algos/LogReg_clf.pickle')

# Stochastic Gradient Descent Classifier
SGD_Clf = load_model(f'{Sentiment_models}/pickled_algos/SGD_clf.pickle')

# Initializing the ensemble classifier 
ensemble_clf = EnsembleClassifier(ONB_Clf, MNB_Clf, BNB_Clf, LogReg_Clf, SGD_Clf)

# List of only feature dictionary from the featureset list of tuples 
feature_list = [f[0] for f in testing_set]

# Looping over each to classify each review
ensemble_preds = [ensemble_clf.classify(features) for features in feature_list]
    
f1_score(ground_truth, ensemble_preds, average = 'micro')

def sentiment(text):
    feats = find_features(text)
    return ensemble_clf.classify(feats), ensemble_clf.confidence(feats)   

def sentiment_out_class(text):
    feats = find_features(text)
    return ensemble_clf.classify(feats)

def sentiment_out_conf(text):
    feats = find_features(text)
    return ensemble_clf.confidence(feats)   

other_data = pd.DataFrame(other_data, columns=["date", "title", "type", "segment", "interest", "increase", "decrease", "good", "bad", "neutral"])

advanced_sent = []

print('Start Advanced Sentiment Analysis')
for index, row in df.iterrows():
    date = row["date"]
    title = row["title"]
    type = row["type"]
    segment = row["segment"]
    feats = find_features(segment)
    seg_class = ensemble_clf.classify(feats)
    seg_conf = ensemble_clf.confidence(feats)  
    
    advanced_sent.append([date, title, type, segment, seg_class, seg_conf])
    
out_advanced_sent = pd.DataFrame(advanced_sent, columns=["date", "title", "type", "segment", "sentiment", "confidence"])
print(out_advanced_sent.head())
out_advanced_sent.to_csv(f"{Sentiment_models}/{Body}_{Model}_advanced_sentiment_texts.csv")

advanced_sent_2 = []

for index, row in df.iterrows():
    date = row["date"]
    title = row["title"]
    type = row["type"]
    segment = row["segment"]
    feats = find_features(segment)
    seg_class = ensemble_clf.classify(feats)
    seg_conf = ensemble_clf.confidence(feats)  
    
    advanced_sent_2.append([date, title, type, segment, seg_class, seg_conf])
    
out_advanced_sent_2 = pd.DataFrame(advanced_sent_2, columns=["date", "title", "type", "segment", "sentiment", "confidence"])
print(out_advanced_sent_2.head())
out_advanced_sent_2.to_csv(f"{Sentiment_models}/{Body_2}_{Model_2}_advanced_sentiment_texts.csv")

print('Finshed Advanced Sentiment Analysis')
exit()

df_out = pd.DataFrame(out, columns=["date", "title", "type", "segment", "interest", "increase", "decrease", "good", "bad", "neutral"])
df_out.to_csv(f"{Model_Folder}/{Model}_sentiment_texts.csv")  