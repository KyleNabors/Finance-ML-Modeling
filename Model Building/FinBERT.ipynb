{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch \n",
    "import nltk\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and import config file\n",
    "config_path = os.getcwd()\n",
    "sys.path.append(config_path)\n",
    "import config\n",
    "\n",
    "#Variables, Paramaters, and Pathnames needed for this script\n",
    "database_file = config.database\n",
    "database_folder = config.database_folder\n",
    "bert_models = config.bert_models\n",
    "bert_models_local = config.bert_models_local\n",
    "keywords = config.keywords\n",
    "finbert_models = config.finbert_models\n",
    "\n",
    "Body = config.Body\n",
    "Model = config.Model\n",
    "Model_Subfolder = f'/{Body} Texts/{Model}'\n",
    "Model_Folder = config.texts\n",
    "Model_Folder = Model_Folder + Model_Subfolder\n",
    "\n",
    "df = pd.read_csv(f\"{Model_Folder}/{Model}_texts_long.csv\")  \n",
    "df = df[df['language'] == 'en']\n",
    "df_2 = pd.read_csv(f\"{Model_Folder}/{Model}_texts.csv\")  \n",
    "df_2 = df_2[df_2['language'] == 'en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finbert \n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer_2 = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer_2)\n",
    "\n",
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "out_1= []\n",
    "out_2 = []\n",
    "sent_val = list()\n",
    "tone_val = list()\n",
    "long = 0\n",
    "errors = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_2.iterrows():\n",
    "    docs = row[\"segment\"]\n",
    "    timestamps = row['date']\n",
    "    type = row['type']\n",
    "    title = row['title']\n",
    "    docs = str(docs)\n",
    "    \n",
    "    try:\n",
    "            results = nlp(docs)\n",
    "    except:\n",
    "            errors += 1\n",
    "            continue\n",
    "    \n",
    "    results = results[0]['label']\n",
    "    if results == \"Negative\":\n",
    "        r_num = -1 \n",
    "    if results == \"Neutral\":\n",
    "        r_num = 0\n",
    "    if results == \"Positive\":\n",
    "        r_num = 1\n",
    "\n",
    "    out_1.append([timestamps, title, r_num])\n",
    "    \n",
    "df_out_1 = pd.DataFrame(out_1, columns=[\"date\", \"title\", \"sentiment\"])\n",
    "df_out_1_2 = df_out_1[[\"title\", \"sentiment\"]]\n",
    "df_out_1_2 = df_out_1_2.groupby(['title']).mean()\n",
    "\n",
    "print(df_out_1.head())\n",
    "\n",
    "print(f'The analysis failed {errors} times.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    docs = row[\"segment\"]\n",
    "    timestamps = row['date']\n",
    "    type = row['type']\n",
    "    title = row['title']\n",
    "    docs = str(docs)\n",
    "    \n",
    "    inputs_2 = tokenizer_2(docs, return_tensors=\"pt\", padding=True, truncation=True, max_length=511)\n",
    "    outputs_2 = finbert(**inputs_2)[0]\n",
    "    val_2 = labels[np.argmax(outputs_2.detach().numpy())]\n",
    "    #tone_val.append(val_2)\n",
    "\n",
    "    out_2.append([timestamps, title, type, docs, val_2])\n",
    "\n",
    "df_out_2 = pd.DataFrame(out_2, columns=[\"date\", \"title\", \"type\", \"segment\", \"tone\"])\n",
    "df_out = df_out_2.merge(df_out_1_2, on='title', how='inner') \n",
    "\n",
    "df_out.to_csv(f\"{finbert_models}/{Body}_{Model}_finbert model.csv\")  \n",
    "df_out_1.to_csv(f\"{finbert_models}/{Body}_{Model}_finbert model_line.csv\")  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
