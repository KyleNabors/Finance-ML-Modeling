import os 
import sys
import pandas as pd
import numpy as np

from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from hdbscan import HDBSCAN
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration
from bertopic.vectorizers import ClassTfidfTransformer

from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
import torch 
import nltk
import spacy

# NLTK English stopwords
nlp = spacy.load("en_core_web_lg")
nltk.download('stopwords') 
stopwords_en = nltk.corpus.stopwords.words('english')
stopwords_sp = nltk.corpus.stopwords.words('spanish')
stopwords_fr = nltk.corpus.stopwords.words('french')
stopwords_it = nltk.corpus.stopwords.words('italian')
stopwords = stopwords_en + stopwords_sp + stopwords_fr + stopwords_it


#Find and import config file
config_path = os.getcwd()
sys.path.append(config_path)
import config

#Variables, Paramaters, and Pathnames needed for this script
database_file = config.database
database_folder = config.database_folder
bert_models = config.bert_models
bert_models_local = config.bert_models_local
keywords = config.keywords

Body = config.Body
Model = config.Model
Model_Subfolder = f'/{Body} Texts/{Model}'
Model_Folder = config.texts
Model_Folder = Model_Folder + Model_Subfolder
Sent_Model_Folder = config.Sentiment_models

df = pd.read_csv(f"{Sent_Model_Folder}/{Model}_advanced_sentiment_texts_vader.csv") 

dummies = pd.get_dummies(df['sentiment'], dtype=float)
df = df.join(dummies)

df2 = df.groupby('title', as_index = False).mean()
df2['net'] = df2['pos'] - df2['neg']

df.merge(df2, on='title', how='left')
print(df.head())

exit()

docs = df["segment"].to_list()
timestamps = df['date'].to_list()
type = df['type'].to_list()
title = df['title'].to_list()














df_pos = df[df['sentiment'] == 'pos']
docs_pos = df_pos["segment"].to_list()
timestamps_pos = df_pos['date'].to_list()
type_pos = df_pos['type'].to_list()

df_neg = df[df['sentiment'] == 'neg']
docs_neg = df_neg["segment"].to_list()
timestamps_neg = df_neg['date'].to_list()
type_neg = df_neg['type'].to_list()

# Embedding
#embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
embeddings_pos = embedding_model.encode(docs_pos, 
                                    batch_size=64, 
                                    show_progress_bar=True)

embeddings_neg = embedding_model.encode(docs_neg, 
                                    batch_size=64, 
                                    show_progress_bar=True)

#Reduce Dimensionality
umap_model = UMAP(n_neighbors=5, 
                  n_components=2, 
                  metric='cosine', 
                  n_epochs=500,
                  min_dist=0.0, 
                  target_metric_kwds=keywords, 
                  target_weight=0.95, 
                  verbose=True)

# Clustering model
cluster_model = HDBSCAN(min_cluster_size = 10, 
                        min_samples=10,
                        metric = 'euclidean', 
                        cluster_selection_method = 'eom', 
                        prediction_data = True)

#Representation model
representation_model = MaximalMarginalRelevance(diversity=0.4)

#Create UMAP model
vectorizer_model = CountVectorizer(stop_words=stopwords, ngram_range=(1, 3))

ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)

print("Done Preprocessing Data")
# BERTopic model
topic_model_pos = BERTopic(language= 'english',
                       min_topic_size=15,
                       n_gram_range=(1, 3),
                       nr_topics = 64,
                       embedding_model=embedding_model,
                       umap_model=umap_model,
                       hdbscan_model=cluster_model,
                       vectorizer_model=vectorizer_model,
                       ctfidf_model=ctfidf_model,
                       representation_model=representation_model,
                       verbose=True
                       ).fit(docs_pos, embeddings = embeddings_pos)


torch.save(topic_model_pos, f"{bert_models_local}/{Body}/{Model}/topic_model_sent_pos_{Model}.pt")

topic_model_neg = BERTopic(language= 'english',
                       min_topic_size=15,
                       n_gram_range=(1, 3),
                       nr_topics = 64,
                       embedding_model=embedding_model,
                       umap_model=umap_model,
                       hdbscan_model=cluster_model,
                       vectorizer_model=vectorizer_model,
                       ctfidf_model=ctfidf_model,
                       representation_model=representation_model,
                       verbose=True
                       ).fit(docs_neg, embeddings = embeddings_neg)

torch.save(topic_model_neg, f"{bert_models_local}/{Body}/{Model}/topic_model_sent_neg_{Model}.pt")

# Save topic-terms barcharts as HTML file
topic_model_pos.visualize_barchart(top_n_topics = 32, n_words=8).write_html(f"{bert_models}/barchart_pos.html")

topic_model_neg.visualize_barchart(top_n_topics = 32, n_words=8).write_html(f"{bert_models}/barchart_neg.html")

exit()
topics_per_class = topic_model.topics_per_class(docs, classes=type)
topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=32).write_html(f"{bert_models}/topics_per_class.html")

#topics_over_time = topic_model.topics_over_time(docs, timestamps, nr_bins=200)
#save topics over time graph as HTML file
#topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=32).write_html(f"{bert_models}/topics_over_time.html")

# Save intertopic distance map as HTML file
topic_model.visualize_topics().write_html(f"{bert_models}/intertopic_dist_map.html")



# Save documents projection as HTML file
topic_model.visualize_documents(docs).write_html(f"{bert_models}/projections.html")

# Save topics dendrogram as HTML file
topic_model.visualize_hierarchy().write_html(f"{bert_models}/hieararchy.html")

print("All Visuals Done")