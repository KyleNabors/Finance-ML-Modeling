import os
import sys
import json

# import visulizatoin using Plotly library
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls
from plotly.subplots import make_subplots
import matplotlib.colors as mcolors
# For data visualization
from matplotlib.ticker import MaxNLocator
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.gridspec as gridspec
import seaborn as sns

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this


# To view the most frequent words
from wordcloud import WordCloud

# Timer to track the run time or completeion of execution
from tqdm import tqdm

# Data Manipulation
import pandas as pd
import numpy as np

# To use regular expression
import re

# To find frequency of a token occuring in corpus
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Used different type of containers to store collection
from collections import Counter, defaultdict
import itertools
import collections

# use algo related to NLP
import nltk
from nltk import bigrams
from nltk import ngrams
from nltk.corpus import stopwords

# Lematization
import spacy

# To visualize data in graph format
import networkx as nx

# Decode emoji's into text
import demoji

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

#Json Functions
def load_data(file):
    with open(file) as f:
        data = json.load(f)
        return data
    
def write_data(file, data):
    with open(file, 'w', encoding="utf-8") as f:
        json.dump(data, f, indent=4)
        
#Load Data
#stopwords = stopwords.words('english')
dataset = load_data("/Users/kylenabors/Documents/MS-Thesis Data/Database/Fed Data/fed_data_blocks.json")

df = pd.DataFrame(dataset, columns= ['segments'])

tqdm.pandas()

def word_tokenizer(text):

    # Lower Case
    text = text.lower()

    # removes punctuation,numbers and returns list of words
    text = re.sub('[^A-Za-z]+', ' ', text)
    
    # Remove distracting single quotes
    text = re.sub("\'", "", text)

    # Spilt the df
    text = text.split()

    return text

df['segments'] = df['segments'].progress_map(word_tokenizer)
df.head()

# Download stopwords
nltk.download('stopwords')

stopWords_nltk = set(stopwords.words('english'))
print(f"Stop Words in English : \n{ stopWords_nltk}")

def remove_stopwords(text):
    text = [word for word in text if word not in stopWords_nltk]
    return text

df['segments_model'] = df['segments'].progress_map(remove_stopwords)
df.head()

sp = spacy.load("en_core_web_lg")

def lemmatization(text):

    # text = [sp(word).lemma_ for word in text]

    text = " ".join(text)
    token = sp(text)

    text = [word.lemma_ for word in token]
    return text

df['segments_model'] = df['segments_model'].progress_map(lemmatization)
df.head()

def sent_to_words(texts) : 
    final = []
    for text in texts:
        new = gensim.utils.simple_preprocess(text, deacc=True)
        final.append(new)
    return(final)
# deacc – Remove accent marks from tokens using
# min_len (int, optional) – Min length of token (inclusive). 

df['segments'] = df['segments'].progress_map(sent_to_words)
df.head()

import gensim.corpora as corpora
# Create Dictionary
id2word = corpora.Dictionary(df['segments_model'])
# Create Corpus
texts = df['segments_model']
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1][0][:30])

import pyLDAvis.gensim
from pprint import pprint
import gensim

# number of topics
num_topics = 10


# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                    id2word=id2word,
                                    num_topics=num_topics)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

import pyLDAvis.gensim
import pickle
import pyLDAvis
import os
import pandas

# Visualize the topics
#pyLDAvis.enable_notebook()

LDAvis_data_filepath = os.path.join('/ldavis_prepared_'+str(num_topics))
# # this is a bit time consuming - make the if statement True
# # if you want to execute visualization prep yourself

if 1 == 2:
  LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
  with open(LDAvis_data_filepath, 'wb') as f:
    pickle.dump(LDAvis_prepared, f)

  # load the pre-prepared pyLDAvis data from disk
  with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)

#pyLDAvis.save_html(LDAvis_prepared, '/ldavis_prepared_'+ str(num_topics) +'.html')
#LDAvis_prepared

topic_lookup_data = pd.DataFrame((lda_model.print_topics()),columns=['Topic_Number','Top_Keywords'])
topic_lookup_data['Topic_Name'] = ['price', 'remain','estate', 'district', 'sale', 'activity', 'demand', 'report', 'increase', 'contact']
topic_lookup_data = topic_lookup_data[['Topic_Number','Topic_Name','Top_Keywords']]
topic_lookup_data['Top_Keywords'] = topic_lookup_data.Top_Keywords.str\
.replace(r'[^a-z]',' ',regex=True).apply(lambda x: x.split())
topic_lookup_data.style.set_properties(subset=['Top_Keywords'], **{'width': '300px'})

for index,sent in enumerate(lda_model[corpus]):
    topic_num =[]
    topic_details = sorted(sent,key=lambda x: x[1], reverse=True)[:2] # Getting top 2 topics in descending order
    topic_num.append(topic_details[0][0]) # Appending top topic
    if len(topic_details) > 1:
        if topic_details[1][1] > 0.35: # Appending second topic only if it has more than 35% influence on current row
            topic_num.append(topic_details[1][0])
    df.loc[index,'Topic_Number'] = ','.join(str(x) for x in sorted(topic_num))

for index,topic_num in enumerate(df.Topic_Number):
    topic_name_list=[]
    for single_topic_num in topic_num.split(','):
        single_topic_num=int(single_topic_num)
        topic_name_list.append(topic_lookup_data.loc\
                               [topic_lookup_data.Topic_Number == single_topic_num,'Topic_Name'][single_topic_num]) 
        # Extracting topic names from lookup table
        
    df.loc[index,'Topic_Name'] =' & '.join(topic_name_list)